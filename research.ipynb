{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.9.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Downloading lxml-4.9.3-cp311-cp311-macosx_11_0_universal2.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m228.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml # pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "\n",
    "\n",
    "# define headers argument\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "\n",
    "search = \"Attention is all you need\"\n",
    "\n",
    "# format the google scholar article link\n",
    "# 'https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=web+scraping&btnG='\n",
    "base = 'https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q='\n",
    "end = '&btnG='\n",
    "url = base + '+'.join(search.split()) + end\n",
    "\n",
    "response=requests.get(url,headers=headers)\n",
    "\n",
    "# soup query\n",
    "soup=BeautifulSoup(response.content, \"html.parser\") # 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Attention is all you need\n",
      "\n",
      "\n",
      "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
      "… to attend to all positions in the decoder up to and including that position. We need to prevent \n",
      "… We implement this inside of scaled dot-product attention by masking out (setting to −∞) …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Attention is all you need in speech separation\n",
      "\n",
      "\n",
      "https://arxiv.org/pdf/2010.13154\n",
      "… Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent \n",
      "computations with a multi-head attention mechanism.In this paper, we propose the SepFormer…\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Attention is all you need\n",
      "\n",
      "\n",
      "http://www.aiotlab.org/teaching/intro2ai/slides/10_attention_n_bert.pdf\n",
      "… Attention • Visual attention and textual attention … Could we build a transformer-based \n",
      "model whose language model looks both forward and backwards, ie “is conditioned on both …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Channel attention is all you need for video frame interpolation\n",
      "\n",
      "\n",
      "https://aaai.org/ojs/index.php/AAAI/article/view/6693/6547\n",
      "… Since every spatial region is important for pixel-level synthesis, we do not explicitly consider \n",
      "spatial attention. Instead, we adopt the channel attention method proposed in (Zhang et al. …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Attention is not all you need: Pure attention loses rank doubly exponentially with depth\n",
      "\n",
      "\n",
      "http://proceedings.mlr.press/v139/dong21a/dong21a.pdf\n",
      "Attention-based architectures have become ubiquitous in machine learning. Yet, our \n",
      "understanding of the reasons for their effectiveness remains limited. This work proposes a new way …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fastformer: Additive attention can be all you need\n",
      "\n",
      "\n",
      "https://arxiv.org/pdf/2108.09084\n",
      "… attention. In Fastformer, instead of modeling the pair-wise intractions between tokens, we first \n",
      "use additive attention mecha… • We propose an additive attention based Transformer named …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "[PDF][PDF] Is space-time attention all you need for video understanding?\n",
      "\n",
      "\n",
      "http://proceedings.mlr.press/v139/bertasius21a/bertasius21a-supp.pdf\n",
      "… Here, we investigate whether reversing the order of time-… attention (ie, applying spatial \n",
      "attention first, then temporal) has an impact on our results. We report that applying spatial attention …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Attention is all you need: An interpretable transformer-based asset allocation approach\n",
      "\n",
      "\n",
      "https://www.sciencedirect.com/science/article/pii/S1057521923003927\n",
      "… We focus this analysis on unconditional pricing errors, defined as:(17) α i ≔ E u i , t = E r i \n",
      ", t − E r ̂ i , t where α i is the forecast error, which is the expectation of the monthly forecast …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "The nature of attention\n",
      "\n",
      "\n",
      "http://www.littlebang.org/wp-content/uploads/2018/07/Watzl-Attention-Nature-2.pdf\n",
      "… the divergence between what we have learned about attentional … ‘Attention’ might work \n",
      "roughly like a natural kind term: its reference is not completely fixed by folk usage or by what we …\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Coatnet: Marrying convolution and attention for all data sizes\n",
      "\n",
      "\n",
      "https://proceedings.neurips.cc/paper/2021/file/20568692db622456cc42a2e853ca21f8-Paper.pdf\n",
      "… As we have discuss above, the global context has a quadratic complexity wrt the spatial size. \n",
      "Hence, if we directly apply the relative attention … in practice, we have mainly three options: …\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#print(soup)\n",
    "\n",
    "\n",
    "\n",
    "for item in soup.select('[data-lid]'):\n",
    "\ttry:\n",
    "\t\tprint('----------------------------------------')\n",
    "\n",
    "\t\t# title\n",
    "\t\tprint(item.select('h3')[0].get_text())\n",
    "\t\tprint(\"\\n\")\n",
    "\n",
    "\t\t# URL download link for the PDF\n",
    "\t\tprint(item.select('a')[0]['href'])\n",
    "\t\t\n",
    "\t\tprint(item.select('.gs_rs')[0].get_text())\n",
    "\n",
    "\t\tprint('----------------------------------------')\n",
    "\t\t\n",
    "\texcept Exception as e:\n",
    "\t\t#raise e\n",
    "\t\tprint('')\n",
    "\n",
    "# parse the most relevant articles (i.e. top 5?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
