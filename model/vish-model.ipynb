{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d7eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (0.0.330)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (3.8.6)\n",
      "Collecting anyio<4.0 (from langchain)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (0.0.57)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Installing collected packages: anyio\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "Successfully installed anyio-3.7.1\n",
      "Requirement already satisfied: pinecone-client in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (4.8.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (2.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from pinecone-client) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81684d10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdotenv\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      2\u001b[0m load_dotenv()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1241daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db876b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(\"data\")\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76440054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e766d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srikanthsrinivasan/Documents/litpal/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:217: UserWarning: WARNING! model_name is not default parameter.\n",
      "                    model_name was transferred to model_kwargs.\n",
      "                    Please confirm that model_name is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\", openai_api_key=\"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\")\n",
    "\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "len(query_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a9fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=\"b3c95e66-d3bc-41fc-ae63-21dcd3f36212\",\n",
    "    environment=\"us-west4-gcp-free\"\n",
    ")\n",
    "\n",
    "index_name = \"hacksc\"\n",
    "\n",
    "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef6877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_docs(query, k=3, score=False):  # we can control k value to get no. of context with respect to question.\n",
    "  if score:\n",
    "    similar_docs = index.similarity_search_with_score(query, k=k)\n",
    "  else:\n",
    "    similar_docs = index.similarity_search(query, k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df64d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2|{16]. In all but a few cases [22]], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P1O0 GPUs.\\n\\n2 Background', metadata={'source': 'data/file.txt'}),\n",
       " Document(page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2|{16]. In all but a few cases [22]], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P1O0 GPUs.\\n\\n2 Background', metadata={'source': 'data/file.txt'}),\n",
       " Document(page_content='Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.', metadata={'source': 'data/file.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_docs(\"What is a transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e794338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llm import AzureOpenAI\n",
    "model_name = \"text-davinci-003\"\n",
    "# llm = AzureOpenAI(model_name=model_name)\n",
    "\n",
    "# chain = load_qa_chain(llm, chain_type=\"stuff\") #we can use map_reduce chain_type also.\n",
    "\n",
    "def get_answer(query):\n",
    "    similar_docs = get_similar_docs(query)\n",
    "    print(similar_docs)\n",
    "    prompt = f\"\"\"\n",
    "            You are reading a mentor chatbot. The chatbot is designed to help you with questions about the research paper.\n",
    "            Response should be very similar to the original paper. But also simplified to help the mentee understand the paper.\n",
    "            Below is a query I received from the mentee:\n",
    "            {query}\n",
    "\n",
    "            Below is {len(similar_docs)} paragraphs from the document that give context to the solution:\n",
    "        \"\"\"\n",
    "    for docs in similar_docs:\n",
    "        prompt += docs.page_content + \"\\n\"\n",
    "    \n",
    "    prompt += \"\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "        Please write the best response that I should send to this prospect:\n",
    "\n",
    "        \"\"\"\n",
    "    answer = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\n",
    "        \"role\" : \"user\", \"content\" : prompt\n",
    "    }])\n",
    "#   answer = chain.run(input_documents=similar_docs, question=query)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf8cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2|{16]. In all but a few cases [22]], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P1O0 GPUs.\\n\\n2 Background', metadata={'source': 'data/file.txt'}), Document(page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2|{16]. In all but a few cases [22]], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P1O0 GPUs.\\n\\n2 Background', metadata={'source': 'data/file.txt'}), Document(page_content='Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.', metadata={'source': 'data/file.txt'})]\n",
      "[<OpenAIObject at 0x16d1f54f0> JSON: {\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"A Transformer is a model architecture that is used in sequence transduction tasks, such as machine translation. Unlike traditional models that rely on recurrent or convolutional neural networks, the Transformer solely utilizes attention mechanisms to establish dependencies between input and output. By doing so, the Transformer allows for improved parallelization and training efficiency. In fact, it has achieved state-of-the-art results in translation quality after being trained for just twelve hours on eight GPUs. This architecture has demonstrated superior quality and performance compared to existing models, including ensembles, and has even established new single-model state-of-the-art scores in translation tasks.\"\n",
      "  },\n",
      "  \"finish_reason\": \"stop\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "a = get_answer(\"What is a transformer?\")\n",
    "\n",
    "print(a.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56dd7546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Transformer is a model architecture that is used in sequence transduction tasks, such as machine translation. Unlike traditional models that rely on recurrent or convolutional neural networks, the Transformer solely utilizes attention mechanisms to establish dependencies between input and output. By doing so, the Transformer allows for improved parallelization and training efficiency. In fact, it has achieved state-of-the-art results in translation quality after being trained for just twelve hours on eight GPUs. This architecture has demonstrated superior quality and performance compared to existing models, including ensembles, and has even established new single-model state-of-the-art scores in translation tasks.\n"
     ]
    }
   ],
   "source": [
    "for d in a.choices:\n",
    "    print(d[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
