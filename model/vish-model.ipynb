{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d7eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (0.0.330)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (3.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.0.57)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pinecone-client in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (2.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (4.7.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (1.26.16)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from pinecone-client) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vishnuvelayuthan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81684d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1241daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishnuvelayuthan/computer-science/Hackathons/HackSCX-2023/backend/backend-env/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db876b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(\"data\")\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76440054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e766d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishnuvelayuthan/computer-science/Hackathons/HackSCX-2023/backend/backend-env/lib/python3.11/site-packages/langchain/embeddings/openai.py:217: UserWarning: WARNING! model_name is not default parameter.\n",
      "                    model_name was transferred to model_kwargs.\n",
      "                    Please confirm that model_name is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
    "\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "len(query_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a9fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=\"b3c95e66-d3bc-41fc-ae63-21dcd3f36212\",\n",
    "    environment=\"us-west4-gcp-free\"\n",
    ")\n",
    "\n",
    "index_name = \"hacksc\"\n",
    "\n",
    "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef6877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_docs(query, k=3, score=False):  # we can control k value to get no. of context with respect to question.\n",
    "  if score:\n",
    "    similar_docs = index.similarity_search_with_score(query, k=k)\n",
    "  else:\n",
    "    similar_docs = index.similarity_search(query, k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df64d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2|{16]. In all but a few cases [22]], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P1O0 GPUs.\\n\\n2 Background', metadata={'source': 'data/file.txt'}),\n",
       " Document(page_content='Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.', metadata={'source': 'data/file.txt'}),\n",
       " Document(page_content='7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.', metadata={'source': 'data/file.txt'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_docs(\"What is a transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e794338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llm import AzureOpenAI\n",
    "model_name = \"text-davinci-003\"\n",
    "# llm = AzureOpenAI(model_name=model_name)\n",
    "\n",
    "# chain = load_qa_chain(llm, chain_type=\"stuff\") #we can use map_reduce chain_type also.\n",
    "\n",
    "def get_answer(query):\n",
    "    similar_docs = get_similiar_docs(query)\n",
    "    print(similar_docs)\n",
    "    prompt = f\"\"\"\n",
    "            You are reading a mentor chatbot. The chatbot is designed to help you with questions about the research paper.\n",
    "            Response should be very similar to the original paper. But also simplified to help the mentee understand the paper.\n",
    "            Below is a query I received from the mentee:\n",
    "            {query}\n",
    "\n",
    "            Below is {len(similar_docs)} paragraphs from the document that give context to the solution:\n",
    "        \"\"\"\n",
    "    for docs in similar_docs:\n",
    "        prompt += docs.page_content + \"\\n\"\n",
    "    \n",
    "    prompt += \"\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "        Please write the best response that I should send to this prospect:\n",
    "\n",
    "        \"\"\"\n",
    "    answer = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\n",
    "        \"role\" : \"user\", \"content\" : prompt\n",
    "    }])\n",
    "#   answer = chain.run(input_documents=similar_docs, question=query)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf8cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2|{16]. In all but a few cases [22]], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P1O0 GPUs.\\n\\n2 Background', metadata={'source': 'data/file.txt'}), Document(page_content='Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.', metadata={'source': 'data/file.txt'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8HQgebzSXLV8JeMEx9Zx7GW6UqaYx at 0x1744adfd0> JSON: {\n",
       "  \"id\": \"chatcmpl-8HQgebzSXLV8JeMEx9Zx7GW6UqaYx\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1699164852,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"A transformer is a model architecture that uses an attention mechanism to establish connections between input and output sequences without relying on recurrence or convolutions. This allows for more parallelization and faster training time. In comparison to other sequence transduction models, such as those based on recurrent or convolutional neural networks, the Transformer model has shown superior quality in machine translation tasks. It achieves higher BLEU scores and can outperform existing models, including ensembles, by a significant margin. For example, it achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over the existing best results by more than 2 BLEU. It also establishes a new state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task after training for 3.5 days on eight GPUs, which is much less than the training time required by previous top-performing models.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 439,\n",
       "    \"completion_tokens\": 193,\n",
       "    \"total_tokens\": 632\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"What is a transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd7546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-sc",
   "language": "python",
   "name": "hack-sc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
